"""
HDBSCAN Clustering for Journal Entries

This script performs soft clustering on journal entries using HDBSCAN,
allowing each entry to belong to multiple clusters based on membership probabilities.
Embeddings are read from the database (generated by generate_embeddings.py).
Cluster metadata is stored in PostgreSQL.
"""

import os
import sys
import json
import numpy as np
import hdbscan
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import List, Tuple, Optional, Dict
from dataclasses import dataclass
from sqlalchemy.orm import Session
from sqlalchemy import desc
from tqdm import tqdm
from env import load_root_env

# Load environment variables (single consolidated root `.env`)
load_root_env()

# Configure Hugging Face cache directory
HF_HOME = os.getenv("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
os.environ["HF_HOME"] = HF_HOME
os.environ["TRANSFORMERS_CACHE"] = HF_HOME
os.environ["HF_DATASETS_CACHE"] = HF_HOME

# Ensure cache directory exists
os.makedirs(HF_HOME, exist_ok=True)

# Optional visualization imports
try:
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False

# Optional transformers imports for topic labeling
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

# Add parent dir to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from database import SessionLocal, engine, Base
from models import JournalEntry, User, ClusteringRun, Cluster, EntryClusterAssignment


@dataclass
class ClusterAssignment:
    """Represents a cluster assignment for an entry."""
    entry_id: int
    cluster_id: int
    membership_probability: float
    is_primary: bool


@dataclass
class ClusterInfo:
    """Cluster metadata."""
    cluster_id: int
    size: int
    persistence: float
    centroid_entry_id: Optional[int]  # Entry closest to cluster centroid
    topic_label: Optional[str] = None  # AI-generated topic label


class ClusterMetadataDB:
    """PostgreSQL database for storing cluster metadata using SQLAlchemy."""
    
    def __init__(self, session: Optional[Session] = None):
        """
        Initialize the cluster metadata database.
        
        Args:
            session: Optional SQLAlchemy session. If None, creates a new session.
        """
        self.session = session if session else SessionLocal()
        self._own_session = session is None
        self._ensure_tables_exist()
    
    def _ensure_tables_exist(self):
        """Ensure the cluster metadata tables exist in PostgreSQL."""
        # Create tables if they don't exist
        Base.metadata.create_all(bind=engine)
    
    def save_clustering_run(
        self,
        user_id: int,
        num_entries: int,
        num_clusters: int,
        min_cluster_size: int,
        min_samples: Optional[int],
        membership_threshold: float,
        noise_entries: int,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> int:
        """Save a clustering run and return its ID."""
        run = ClusteringRun(
            user_id=user_id,
            run_timestamp=datetime.now(timezone.utc),
            num_entries=num_entries,
            num_clusters=num_clusters,
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            membership_threshold=membership_threshold,
            noise_entries=noise_entries,
            start_date=start_date,
            end_date=end_date
        )
        self.session.add(run)
        self.session.commit()
        self.session.refresh(run)
        return run.id
    
    def save_cluster(
        self,
        run_id: int,
        cluster_id: int,
        size: int,
        persistence: Optional[float],
        centroid_entry_id: Optional[int],
        topic_label: Optional[str] = None
    ):
        """Save cluster metadata."""
        cluster = Cluster(
            run_id=run_id,
            cluster_id=cluster_id,
            size=size,
            persistence=persistence,
            centroid_entry_id=centroid_entry_id,
            topic_label=topic_label
        )
        self.session.add(cluster)
        self.session.commit()
    
    def save_entry_assignment(
        self,
        run_id: int,
        entry_id: int,
        cluster_id: int,
        membership_probability: float,
        is_primary: bool
    ):
        """Save an entry's cluster assignment."""
        assignment = EntryClusterAssignment(
            run_id=run_id,
            entry_id=entry_id,
            cluster_id=cluster_id,
            membership_probability=membership_probability,
            is_primary=is_primary
        )
        self.session.add(assignment)
        self.session.commit()
    
    def batch_save_assignments(self, assignments: List[Tuple]):
        """Batch save multiple assignments for efficiency."""
        assignment_objects = [
            EntryClusterAssignment(
                run_id=run_id,
                entry_id=entry_id,
                cluster_id=cluster_id,
                membership_probability=membership_probability,
                is_primary=is_primary
            )
            for run_id, entry_id, cluster_id, membership_probability, is_primary in assignments
        ]
        self.session.bulk_save_objects(assignment_objects)
        self.session.commit()
    
    def get_entry_clusters(self, entry_id: int, run_id: Optional[int] = None) -> List[Dict]:
        """Get all cluster assignments for an entry."""
        query = self.session.query(EntryClusterAssignment).filter(
            EntryClusterAssignment.entry_id == entry_id
        )
        
        if run_id:
            query = query.filter(EntryClusterAssignment.run_id == run_id)
        else:
            # Get from latest run
            latest_run = self.session.query(ClusteringRun).order_by(
                desc(ClusteringRun.id)
            ).first()
            if latest_run:
                query = query.filter(EntryClusterAssignment.run_id == latest_run.id)
            else:
                return []
        
        assignments = query.order_by(desc(EntryClusterAssignment.membership_probability)).all()
        
        return [
            {
                "cluster_id": a.cluster_id,
                "probability": a.membership_probability,
                "is_primary": a.is_primary
            }
            for a in assignments
        ]
    
    def get_cluster_entries(self, cluster_id: int, run_id: Optional[int] = None) -> List[Dict]:
        """Get all entries in a cluster."""
        query = self.session.query(EntryClusterAssignment).filter(
            EntryClusterAssignment.cluster_id == cluster_id
        )
        
        if run_id:
            query = query.filter(EntryClusterAssignment.run_id == run_id)
        else:
            # Get from latest run
            latest_run = self.session.query(ClusteringRun).order_by(
                desc(ClusteringRun.id)
            ).first()
            if latest_run:
                query = query.filter(EntryClusterAssignment.run_id == latest_run.id)
            else:
                return []
        
        assignments = query.order_by(desc(EntryClusterAssignment.membership_probability)).all()
        
        return [
            {
                "entry_id": a.entry_id,
                "probability": a.membership_probability,
                "is_primary": a.is_primary
            }
            for a in assignments
        ]
    
    def close(self):
        """Close the database session."""
        if self._own_session:
            self.session.close()


class TopicLabeler:
    """Generate topic labels for clusters using SmolLM2."""
    
    def __init__(self, model_name: str = "HuggingFaceTB/SmolLM2-135M-Instruct"):
        """
        Initialize the topic labeler.
        
        Args:
            model_name: Hugging Face model name for topic labeling
        """
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cpu"  # Use CPU by default for lightweight model
    
    def _load_model(self):
        """Lazy load the SmolLM2 model."""
        if self.model is not None:
            return
        
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "transformers not available. Install with: pip install transformers"
            )
        
        print(f"Loading SmolLM2 model: {self.model_name} (HF_HOME={HF_HOME})...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)
        print("SmolLM2 model loaded successfully.")
    
    def generate_topic_label(
        self,
        sample_entries: List[str],
        max_samples: int = 5,
        max_entry_length: int = 300,
        max_new_tokens: int = 10,
        temperature: float = 0.3
    ) -> str:
        """
        Generate a topic label for a cluster based on sample entries.
        
        Args:
            sample_entries: List of journal entry texts from the cluster
            max_samples: Maximum number of samples to use
            max_entry_length: Maximum characters per entry
            max_new_tokens: Maximum tokens to generate for the label
            temperature: Sampling temperature (lower = more deterministic)
        
        Returns:
            A short topic label (2-5 words)
        """
        self._load_model()
        
        # Combine sample entries into a single document
        samples = sample_entries[:max_samples]
        # Truncate long entries to keep prompt manageable
        truncated_samples = [
            s[:max_entry_length] + "..." if len(s) > max_entry_length else s 
            for s in samples
        ]
        
        # Format entries for the prompt
        cluster_entries = "\n\n".join([f"Entry {i+1}: {entry}" for i, entry in enumerate(truncated_samples)])
        
        # Create the prompt
        prompt = f"""You are an expert at labeling topics from journal entries.

Your task is to generate a short, coherent topic title that describes the common theme across the following journal entries.
Requirements:
- The title MUST be 2 to 5 words long
- The title MUST be descriptive and specific
- The title MUST NOT be a sentence
- The title MUST NOT contain punctuation
- The title MUST summarize the shared theme
- The title MUST sound natural and human-readable
- DO NOT explain your answer
- ONLY output the title. DO NOT include any other text or formatting, such as "The Common Theme is:" or "The Topic is:" or "assistant" or any other text.

Journal entries:
{cluster_entries}

Topic title:"""
        
        # Generate topic label
        try:
            messages = [{"role": "user", "content": prompt}]
            input_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = self.tokenizer.encode(input_text, return_tensors="pt").to(self.device)
            
            outputs = self.model.generate(
                inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

            # Decode and extract the label
            full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract just the generated title after the prompt
            # The model output includes the prompt, so we need to extract just the new text
            if "Topic title:" in full_output:
                label = full_output.split("Topic title:")[-1].strip()
            else:
                label = full_output[len(input_text):].strip()

            # Clean up the label
            label = label.split("\n")[1].strip()  # Take only second line (get rid of first line which is "assistant")
            label = label.strip('.,!?;:"\'')  # Remove punctuation
            #remove the string "The Common Theme is:", if it exists
            if "The Common Theme is:" in label:
                label = label.replace("The Common Theme is:", "").strip()
            print(f"Label: {label}")
            
            # Validate length (2-5 words)
            # words = label.split()
            # if len(words) < 2:
            #     return "Unlabeled Cluster, length too short"
            # elif len(words) > 5:
            #     label = " ".join(words[:5])
            
            return label if label else "Unlabeled Cluster, no label generated"
            
        except Exception as e:
            print(f"Error generating label: {e}")
            return "Unlabeled Cluster, error generating label"
    
    def generate_labels_for_clusters(
        self,
        cluster_samples: Dict[int, List[str]],
        show_progress: bool = True
    ) -> Dict[int, str]:
        """
        Generate topic labels for multiple clusters.
        
        Args:
            cluster_samples: Dict mapping cluster_id to list of sample entry texts
            show_progress: Whether to show progress
        
        Returns:
            Dict mapping cluster_id to topic label
        """
        labels = {}
        
        cluster_ids = list(cluster_samples.keys())
        if show_progress:
            print(f"\nGenerating topic labels for {len(cluster_ids)} clusters...")
        
        for i, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):
            if show_progress:
                print(f"  [{i+1}/{len(cluster_ids)}] Cluster {cluster_id}...", end=" ", flush=True)
            
            try:
                label = self.generate_topic_label(cluster_samples[cluster_id])
                labels[cluster_id] = label
                if show_progress:
                    print(f"â†’ {label}")
            except Exception as e:
                print(f"Error: {e}")
                labels[cluster_id] = f"Cluster {cluster_id}"
        
        return labels


class JournalClusterer:
    """HDBSCAN-based clusterer for journal entries with soft clustering support."""
    
    def __init__(
        self,
        min_cluster_size: int = 5,
        min_samples: int = 2,
        membership_threshold: float = 0.1,
        cluster_selection_epsilon: float = 0.0,
        metric: str = 'euclidean',
        cluster_selection_method: str = 'leaf',
        use_umap: bool = True,
        umap_n_components: int = 10,
        umap_n_neighbors: int = 15,
        umap_min_dist: float = 0.0,
        umap_metric: str = 'cosine'
    ):
        """
        Initialize the clusterer.
        
        Args:
            min_cluster_size: Minimum number of points to form a cluster.
                             For journal entries, 3-5 works well since themes may be specific.
            min_samples: Minimum samples in a neighborhood. Lower values (1-2) allow more 
                        points to be considered core points. Default: 2.
            membership_threshold: Minimum probability to assign an entry to a cluster.
                                 Lower values (0.05-0.1) allow more multi-cluster assignments.
            cluster_selection_epsilon: Distance threshold for cluster merging. 
                                      Use 0.0 to disable merging (recommended).
            metric: Distance metric for HDBSCAN.
            cluster_selection_method: 'leaf' for fine-grained clusters, 'eom' for larger clusters.
                                     'leaf' is recommended for journal entries.
            use_umap: Whether to use UMAP dimensionality reduction before clustering.
                     Highly recommended for high-dimensional embeddings (default: True).
            umap_n_components: Number of dimensions to reduce to (default: 10).
            umap_n_neighbors: UMAP neighborhood size (default: 15).
            umap_min_dist: UMAP minimum distance, 0.0 allows tighter clusters (default: 0.0).
            umap_metric: UMAP distance metric, 'cosine' works well for text (default: 'cosine').
        """
        self.min_cluster_size = min_cluster_size
        self.min_samples = min_samples
        self.membership_threshold = membership_threshold
        self.cluster_selection_epsilon = cluster_selection_epsilon
        self.metric = metric
        self.cluster_selection_method = cluster_selection_method
        self.use_umap = use_umap
        self.umap_n_components = umap_n_components
        self.umap_n_neighbors = umap_n_neighbors
        self.umap_min_dist = umap_min_dist
        self.umap_metric = umap_metric
        self.clusterer = None
        self.soft_clusters = None
        self.umap_reducer = None
        self.reduced_embeddings = None
        
    def fit(self, embeddings: np.ndarray) -> 'JournalClusterer':
        """
        Fit HDBSCAN to the embeddings.
        
        Args:
            embeddings: Array of shape (n_samples, n_features)
        
        Returns:
            self
        """
        # Apply UMAP dimensionality reduction if enabled
        if self.use_umap:
            if not UMAP_AVAILABLE:
                print("WARNING: UMAP requested but not available. Install with: pip install umap-learn")
                print("Falling back to direct HDBSCAN clustering...")
                clustering_data = embeddings
            else:
                print(f"Applying UMAP: {embeddings.shape[1]}D -> {self.umap_n_components}D "
                      f"(metric={self.umap_metric}, n_neighbors={self.umap_n_neighbors})")
                self.umap_reducer = umap.UMAP(
                    n_components=self.umap_n_components,
                    n_neighbors=self.umap_n_neighbors,
                    min_dist=self.umap_min_dist,
                    metric=self.umap_metric,
                    random_state=42
                )
                self.reduced_embeddings = self.umap_reducer.fit_transform(embeddings)
                clustering_data = self.reduced_embeddings
                print(f"UMAP reduction complete: {clustering_data.shape}")
        else:
            clustering_data = embeddings
        
        print(f"Fitting HDBSCAN with min_cluster_size={self.min_cluster_size}, "
              f"min_samples={self.min_samples}, metric={self.metric}, "
              f"selection_method={self.cluster_selection_method}")
        
        self.clusterer = hdbscan.HDBSCAN(
            min_cluster_size=self.min_cluster_size,
            min_samples=self.min_samples,
            cluster_selection_epsilon=self.cluster_selection_epsilon,
            metric=self.metric,
            cluster_selection_method=self.cluster_selection_method,
            prediction_data=True,  # Required for soft clustering
            gen_min_span_tree=True  # For visualization if needed
        )
        
        self.clusterer.fit(clustering_data)
        
        # Compute soft cluster memberships
        print("Computing soft cluster memberships...")
        self.soft_clusters = hdbscan.all_points_membership_vectors(self.clusterer)
        
        # Summary statistics
        labels = self.clusterer.labels_
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        n_clustered = len(labels) - n_noise
        
        print(f"Found {n_clusters} clusters")
        print(f"Clustered: {n_clustered} entries ({100*n_clustered/len(labels):.1f}%)")
        print(f"Noise points: {n_noise} ({100*n_noise/len(labels):.1f}%)")
        
        return self
    
    def get_hard_labels(self) -> np.ndarray:
        """Get hard cluster assignments (single cluster per point)."""
        if self.clusterer is None:
            raise ValueError("Must call fit() first")
        return self.clusterer.labels_
    
    def get_soft_memberships(self) -> np.ndarray:
        """Get soft cluster membership probabilities."""
        if self.soft_clusters is None:
            raise ValueError("Must call fit() first")
        return self.soft_clusters
    
    def get_multi_cluster_assignments(
        self, 
        entry_indices: List[int],
        entry_ids: List[int]
    ) -> List[ClusterAssignment]:
        """
        Get multiple cluster assignments per entry based on membership threshold.
        
        Args:
            entry_indices: Indices into the embeddings array
            entry_ids: Database IDs of the entries
        
        Returns:
            List of ClusterAssignment objects
        """
        if self.soft_clusters is None:
            raise ValueError("Must call fit() first")
        
        assignments = []
        hard_labels = self.clusterer.labels_
        
        for idx, entry_id in zip(entry_indices, entry_ids):
            memberships = self.soft_clusters[idx]
            primary_cluster = int(hard_labels[idx])  # Convert numpy.int64 to Python int
            
            # Handle case where memberships is a scalar (single cluster or no clusters)
            if np.isscalar(memberships):
                memberships = np.array([memberships])
            
            # Get all clusters above threshold
            for cluster_id, prob in enumerate(memberships):
                if prob >= self.membership_threshold:
                    assignments.append(ClusterAssignment(
                        entry_id=entry_id,
                        cluster_id=int(cluster_id),  # Convert to Python int
                        membership_probability=float(prob),
                        is_primary=(int(cluster_id) == primary_cluster)
                    ))
            
            # If noise point (label=-1), still record it with special cluster_id
            if primary_cluster == -1:
                assignments.append(ClusterAssignment(
                    entry_id=entry_id,
                    cluster_id=-1,  # Noise cluster
                    membership_probability=1.0,
                    is_primary=True
                ))
        
        return assignments
    
    def get_cluster_info(self, embeddings: np.ndarray, entry_ids: List[int]) -> List[ClusterInfo]:
        """Get information about each cluster."""
        if self.clusterer is None:
            raise ValueError("Must call fit() first")
        
        # Use reduced embeddings if UMAP was applied
        if self.reduced_embeddings is not None:
            working_embeddings = self.reduced_embeddings
        else:
            working_embeddings = embeddings
        
        labels = self.clusterer.labels_
        unique_labels = set(labels) - {-1}  # Exclude noise
        cluster_infos = []
        
        for cluster_id in sorted(unique_labels):
            cluster_id = int(cluster_id)  # Convert numpy.int64 to Python int
            mask = labels == cluster_id
            cluster_embeddings = working_embeddings[mask]
            cluster_entry_ids = [eid for eid, m in zip(entry_ids, mask) if m]
            
            # Calculate centroid
            centroid = cluster_embeddings.mean(axis=0)
            
            # Find entry closest to centroid
            distances = np.linalg.norm(cluster_embeddings - centroid, axis=1)
            closest_idx = np.argmin(distances)
            centroid_entry_id = cluster_entry_ids[closest_idx]
            
            # Get cluster persistence (stability) if available
            persistence = None
            if hasattr(self.clusterer, 'cluster_persistence_'):
                if cluster_id < len(self.clusterer.cluster_persistence_):
                    persistence = float(self.clusterer.cluster_persistence_[cluster_id])
            
            cluster_infos.append(ClusterInfo(
                cluster_id=cluster_id,
                size=int(mask.sum()),
                persistence=persistence,
                centroid_entry_id=centroid_entry_id
            ))
        
        return cluster_infos


def load_user_entries_with_embeddings(
    user_id: int, 
    db: Session, 
    limit: Optional[int] = None,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None
) -> List[JournalEntry]:
    """
    Load journal entries for a user that have embeddings.
    
    Args:
        user_id: User ID to filter by
        db: Database session
        limit: Maximum number of entries to load (default: None for all)
        start_date: Optional start date to filter entries (inclusive)
        end_date: Optional end date to filter entries (inclusive)
    
    Returns:
        List of JournalEntry objects with embeddings
    """
    query = db.query(JournalEntry).filter(
        JournalEntry.user_id == user_id,
        JournalEntry.embedding.isnot(None)
    )
    
    # Apply date filters if provided
    if start_date is not None:
        query = query.filter(JournalEntry.created_at >= start_date)
    if end_date is not None:
        # Add one day to end_date to make it inclusive of the entire day
        end_date_inclusive = end_date + timedelta(days=1)
        query = query.filter(JournalEntry.created_at < end_date_inclusive)
    
    query = query.order_by(JournalEntry.created_at)
    
    if limit is not None:
        query = query.limit(limit)
    
    return query.all()


def get_embeddings_from_entries(entries: List[JournalEntry]) -> Tuple[np.ndarray, List[int]]:
    """
    Extract embeddings from journal entries.
    
    Args:
        entries: List of JournalEntry objects with embeddings
    
    Returns:
        Tuple of (embeddings array with shape (n_entries, 384), list of entry IDs)
    """
    embeddings = []
    entry_ids = []
    
    for entry in entries:
        if entry.embedding is not None:
            emb_data = entry.embedding
            # Handle pgvector vector type (numpy array), list, or string (legacy JSON)
            if isinstance(emb_data, np.ndarray):
                embeddings.append(emb_data.astype(np.float32))
            elif isinstance(emb_data, list):
                embeddings.append(np.array(emb_data, dtype=np.float32))
            elif isinstance(emb_data, str):
                # Legacy JSON string format (for migration compatibility)
                emb_data = json.loads(emb_data)
                embeddings.append(np.array(emb_data, dtype=np.float32))
            else:
                # Try to convert to numpy array
                embeddings.append(np.array(list(emb_data), dtype=np.float32))
            entry_ids.append(entry.id)
    
    if not embeddings:
        return np.array([]).reshape(0, 384), []
    
    # Stack into 2D array: shape (n_entries, embedding_dim)
    return np.vstack(embeddings), entry_ids


def analyze_entry_lengths(entries: List[JournalEntry]) -> Dict:
    """Analyze entry lengths to help tune clustering parameters."""
    lengths = [len(entry.content) for entry in entries]
    word_counts = [len(entry.content.split()) for entry in entries]
    
    return {
        "count": len(entries),
        "char_length": {
            "min": min(lengths),
            "max": max(lengths),
            "mean": np.mean(lengths),
            "median": np.median(lengths),
            "std": np.std(lengths)
        },
        "word_count": {
            "min": min(word_counts),
            "max": max(word_counts),
            "mean": np.mean(word_counts),
            "median": np.median(word_counts),
            "std": np.std(word_counts)
        },
        "long_entries": sum(1 for wc in word_counts if wc > 200),
        "medium_entries": sum(1 for wc in word_counts if 50 <= wc <= 200),
        "short_entries": sum(1 for wc in word_counts if wc < 50)
    }


def run_clustering(
    user_id: int = 1,
    min_cluster_size: Optional[int] = None,
    min_samples: Optional[int] = None,
    membership_threshold: Optional[float] = None,
    cluster_selection_epsilon: Optional[float] = None,
    metric: str = 'euclidean',
    cluster_selection_method: str = 'leaf',
    use_umap: bool = True,
    umap_n_components: int = 10,
    umap_n_neighbors: Optional[int] = None,
    umap_min_dist: Optional[float] = None,
    umap_metric: str = 'cosine',
    generate_topics: bool = False,
    topic_model: str = "HuggingFaceTB/SmolLM2-135M-Instruct",
    metadata_db: Optional[str] = None,
    limit: Optional[int] = None,
    visualize: bool = False,
    output_dir: str = ".",
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None
):
    """
    Run HDBSCAN clustering on a user's journal entries.
    
    Embeddings must be pre-generated using embeddings.py.
    
    Recommended settings for journal entries:
    - use_umap=True with n_components=10 (significantly improves clustering)
    - cluster_selection_method='leaf' for fine-grained thematic clusters
    - min_samples=2 to allow more points to be clustered
    - cluster_selection_epsilon=0.0 to disable automatic cluster merging
    
    Args:
        user_id: User ID to cluster entries for
        min_cluster_size: Minimum cluster size (default: 5)
        min_samples: Minimum samples in neighborhood (default: 2)
        membership_threshold: Minimum probability for soft assignment (default: 0.1)
        cluster_selection_epsilon: Distance for cluster merging (default: 0.0)
        metric: Distance metric for HDBSCAN (default: 'euclidean')
        cluster_selection_method: 'leaf' for fine-grained, 'eom' for larger clusters
        use_umap: Whether to use UMAP dimensionality reduction (default: True)
        umap_n_components: UMAP output dimensions (default: 10)
        umap_metric: UMAP distance metric (default: 'cosine')
        generate_topics: Whether to generate topic labels using SmolLM2 (default: False)
        topic_model: Hugging Face model name for topic labeling (default: "HuggingFaceTB/SmolLM2-135M-Instruct")
        metadata_db: Deprecated - metadata is now stored in PostgreSQL
        limit: Maximum number of entries to load (None for all)
        visualize: Whether to generate visualization plots
        output_dir: Directory for output files
        start_date: Optional start date to filter entries (inclusive)
        end_date: Optional end date to filter entries (inclusive)
    """
    print(f"\n{'='*60}")
    print(f"HDBSCAN Clustering for User {user_id}")
    print(f"{'='*60}\n")
    
    # Log all parameters at the start
    print("--- Clustering Run Parameters ---")
    print(f"User ID: {user_id}")
    if start_date:
        print(f"Start date: {start_date.strftime('%Y-%m-%d %H:%M:%S')}")
    if end_date:
        print(f"End date: {end_date.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"min_cluster_size: {min_cluster_size}")
    print(f"min_samples: {min_samples}")
    print(f"membership_threshold: {membership_threshold}")
    print(f"cluster_selection_epsilon: {cluster_selection_epsilon}")
    print(f"metric: {metric}")
    print(f"cluster_selection_method: {cluster_selection_method}")
    print(f"use_umap: {use_umap}")
    if use_umap:
        print(f"umap_n_components: {umap_n_components}")
        print(f"umap_n_neighbors: {umap_n_neighbors}")
        print(f"umap_min_dist: {umap_min_dist}")
        print(f"umap_metric: {umap_metric}")
    print("="*60)
    print()
    
    # Load entries
    db = SessionLocal()
    try:
        # Check if user exists
        user = db.query(User).filter(User.id == user_id).first()
        if user:
            print(f"User: {user.email} (ID: {user.id})")
        else:
            print(f"Warning: User ID {user_id} not found, checking entries directly...")
        
        # Load entries that have embeddings
        entries = load_user_entries_with_embeddings(user_id, db, limit=limit, start_date=start_date, end_date=end_date)
        
        # Print date filter info if provided
        if start_date or end_date:
            date_range_str = ""
            if start_date:
                date_range_str += f"from {start_date.strftime('%Y-%m-%d')} "
            if end_date:
                date_range_str += f"to {end_date.strftime('%Y-%m-%d')}"
            print(f"Filtering entries {date_range_str}")
        
        if not entries:
            print(f"No entries with embeddings found for user {user_id}")
            print("Run generate_embeddings.py first to generate embeddings.")
            return
        
        print(f"\nLoaded {len(entries)} journal entries with embeddings")
        
        # Get embeddings from database
        embeddings, entry_ids = get_embeddings_from_entries(entries)
        
        if len(embeddings) == 0:
            print("No embeddings found. Run generate_embeddings.py first.")
            return
        
        print(f"Embeddings shape: {embeddings.shape}")
        
        # Analyze entry lengths
        print("\n--- Entry Length Analysis ---")
        length_stats = analyze_entry_lengths(entries)
        print(f"Total entries: {length_stats['count']}")
        print(f"Word count - Mean: {length_stats['word_count']['mean']:.0f}, "
              f"Median: {length_stats['word_count']['median']:.0f}, "
              f"Range: {length_stats['word_count']['min']}-{length_stats['word_count']['max']}")
        print(f"Entry distribution:")
        print(f"  Long (>200 words): {length_stats['long_entries']}")
        print(f"  Medium (50-200 words): {length_stats['medium_entries']}")
        print(f"  Short (<50 words): {length_stats['short_entries']}")
        
        # Set final parameters with sensible defaults
        final_min_cluster_size = min_cluster_size if min_cluster_size is not None else 5
        final_min_samples = min_samples if min_samples is not None else 2
        final_membership_threshold = membership_threshold if membership_threshold is not None else 0.1
        final_cluster_selection_epsilon = cluster_selection_epsilon if cluster_selection_epsilon is not None else 0.0
        final_umap_n_components = umap_n_components if umap_n_components is not None else 10
        final_umap_n_neighbors = umap_n_neighbors if umap_n_neighbors is not None else 15
        final_umap_min_dist = umap_min_dist if umap_min_dist is not None else 0.0
        final_metric = metric
        
        print(f"\n--- Using Parameters ---")
        print(f"min_cluster_size: {final_min_cluster_size}")
        print(f"min_samples: {final_min_samples}")
        print(f"membership_threshold: {final_membership_threshold}")
        print(f"cluster_selection_epsilon: {final_cluster_selection_epsilon}")
        print(f"cluster_selection_method: {cluster_selection_method}")
        print(f"metric: {final_metric}")
        print(f"use_umap: {use_umap}")
        if use_umap:
            print(f"umap_n_components: {final_umap_n_components}")
            print(f"umap_n_neighbors: {final_umap_n_neighbors}")
            print(f"umap_min_dist: {final_umap_min_dist}")
            print(f"umap_metric: {umap_metric}")
        
        # Run clustering
        print("\n--- Running HDBSCAN Clustering ---")
        clusterer = JournalClusterer(
            min_cluster_size=final_min_cluster_size,
            min_samples=final_min_samples,
            membership_threshold=final_membership_threshold,
            cluster_selection_epsilon=final_cluster_selection_epsilon,
            metric=final_metric,
            cluster_selection_method=cluster_selection_method,
            use_umap=use_umap,
            umap_n_components=final_umap_n_components,
            umap_n_neighbors=final_umap_n_neighbors,
            umap_min_dist=final_umap_min_dist,
            umap_metric=umap_metric
        )
        
        clusterer.fit(embeddings)
        
        # Get hard labels and cluster info
        hard_labels = clusterer.get_hard_labels()
        soft_clusters = clusterer.get_soft_memberships()
        cluster_infos = clusterer.get_cluster_info(embeddings, entry_ids)
        
        # Calculate statistics
        n_clusters = len(set(hard_labels)) - (1 if -1 in hard_labels else 0)
        n_noise = list(hard_labels).count(-1)
        n_clustered = len(hard_labels) - n_noise
        
        print("\n" + "="*60)
        print("CLUSTERING RESULTS SUMMARY")
        print("="*60)
        
        print(f"\n--- Hard Clustering (Primary Assignment) ---")
        print(f"Total entries: {len(entry_ids)}")
        print(f"Clusters found: {n_clusters}")
        print(f"Entries in clusters: {n_clustered} ({100*n_clustered/len(entry_ids):.1f}%)")
        print(f"Noise (unassigned): {n_noise} ({100*n_noise/len(entry_ids):.1f}%)")
        
        print(f"\n--- Cluster Details ---")
        for info in cluster_infos:
            persistence_str = f"{info.persistence:.3f}" if info.persistence else "N/A"
            print(f"  Cluster {info.cluster_id}: {info.size} entries, "
                  f"persistence={persistence_str}, "
                  f"centroid_entry={info.centroid_entry_id}")
        
        # Get multi-cluster assignments
        indices = list(range(len(entry_ids)))
        assignments = clusterer.get_multi_cluster_assignments(indices, entry_ids)
        
        # Count soft clustering stats
        entries_assignment_count = {}
        non_noise_assignments = []
        for a in assignments:
            if a.cluster_id != -1:  # Exclude noise assignments
                non_noise_assignments.append(a)
            if a.entry_id not in entries_assignment_count:
                entries_assignment_count[a.entry_id] = {"total": 0, "non_noise": 0}
            entries_assignment_count[a.entry_id]["total"] += 1
            if a.cluster_id != -1:
                entries_assignment_count[a.entry_id]["non_noise"] += 1
        
        # Count entries with soft assignments to actual clusters (not noise)
        entries_with_cluster_membership = sum(
            1 for counts in entries_assignment_count.values() 
            if counts["non_noise"] > 0
        )
        entries_with_multi_cluster = sum(
            1 for counts in entries_assignment_count.values() 
            if counts["non_noise"] > 1
        )
        
        print(f"\n--- Soft Clustering (Membership Probabilities >= {final_membership_threshold}) ---")
        print(f"Entries with ANY cluster membership: {entries_with_cluster_membership} "
              f"({100*entries_with_cluster_membership/len(entry_ids):.1f}%)")
        print(f"Entries with MULTIPLE cluster membership: {entries_with_multi_cluster} "
              f"({100*entries_with_multi_cluster/len(entry_ids):.1f}%)")
        print(f"Total soft assignments (excluding noise): {len(non_noise_assignments)}")
        
        # Generate topic labels if requested
        entry_map = {e.id: e for e in entries}
        topic_labels = {}
        
        if generate_topics:
            if not TRANSFORMERS_AVAILABLE:
                print("\nWARNING: transformers not available. Install with: pip install transformers")
                print("Skipping topic generation...")
            else:
                print("\n--- Generating Topic Labels ---")
                
                # Collect sample entries for each cluster
                cluster_samples = {}
                for info in cluster_infos:
                    # Get entries in this cluster (primary assignment)
                    cluster_entry_ids = [
                        a.entry_id for a in assignments
                        if a.cluster_id == info.cluster_id and a.is_primary
                    ]
                    # Get top 5 entry contents
                    sample_contents = []
                    for eid in cluster_entry_ids[:5]:
                        if eid in entry_map:
                            sample_contents.append(entry_map[eid].content)
                    cluster_samples[info.cluster_id] = sample_contents
                
                # Generate labels
                labeler = TopicLabeler(model_name=topic_model)
                topic_labels = labeler.generate_labels_for_clusters(cluster_samples)
                
                # Update cluster_infos with topic labels
                for info in cluster_infos:
                    info.topic_label = topic_labels.get(info.cluster_id)
        
        # Save to metadata database
        print("\n--- Saving to Metadata Database ---")
        meta_db = ClusterMetadataDB()
        
        run_id = meta_db.save_clustering_run(
            user_id=user_id,
            num_entries=len(entries),
            num_clusters=n_clusters,
            min_cluster_size=final_min_cluster_size,
            min_samples=final_min_samples,
            membership_threshold=final_membership_threshold,
            noise_entries=n_noise,
            start_date=start_date,
            end_date=end_date
        )
        print(f"Created clustering run ID: {run_id}")
        
        # Save cluster info
        for info in cluster_infos:
            meta_db.save_cluster(
                run_id=run_id,
                cluster_id=info.cluster_id,
                size=info.size,
                persistence=info.persistence,
                centroid_entry_id=info.centroid_entry_id,
                topic_label=info.topic_label
            )
        
        # Batch save assignments
        assignment_tuples = [
            (run_id, a.entry_id, a.cluster_id, a.membership_probability, a.is_primary)
            for a in assignments
        ]
        meta_db.batch_save_assignments(assignment_tuples)
        print(f"Saved {len(assignments)} cluster assignments")
        
        meta_db.close()
        
        # Show sample entries from each cluster
        print("\n--- Sample Entries per Cluster ---")
        
        for info in cluster_infos[:5]:  # Show first 5 clusters
            topic_str = f" - {info.topic_label}" if info.topic_label else ""
            print(f"\n[Cluster {info.cluster_id}]{topic_str} ({info.size} entries)")
            
            # Get entries in this cluster (primary assignment)
            cluster_entries = [
                (a.entry_id, a.membership_probability)
                for a in assignments
                if a.cluster_id == info.cluster_id and a.is_primary
            ]
            cluster_entries.sort(key=lambda x: x[1], reverse=True)
            
            # Show top 2 entries
            for entry_id, prob in cluster_entries[:2]:
                if entry_id in entry_map:
                    content = entry_map[entry_id].content[:150].replace('\n', ' ')
                    print(f"  [{prob:.2f}] Entry {entry_id}: {content}...")
        
        # Generate visualization if requested
        if visualize:
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            # Main cluster visualization
            viz_path = os.path.join(output_dir, f"cluster_viz_user{user_id}.png")
            visualize_clusters(
                embeddings=embeddings,
                labels=hard_labels,
                entry_ids=entry_ids,
                soft_clusters=soft_clusters,
                output_path=viz_path,
                title=f"Journal Entry Clusters (User {user_id})\n{n_clusters} clusters, {n_noise} noise points"
            )
            
            # Condensed tree visualization
            tree_path = os.path.join(output_dir, f"condensed_tree_user{user_id}.png")
            visualize_condensed_tree(clusterer.clusterer, output_path=tree_path)
        
        print(f"\n{'='*60}")
        print("Clustering complete!")
        print(f"Metadata saved to PostgreSQL")
        if visualize:
            print(f"Visualizations saved to: {output_dir}")
        print(f"{'='*60}\n")
        
    finally:
        db.close()


def visualize_clusters(
    embeddings: np.ndarray,
    labels: np.ndarray,
    entry_ids: List[int],
    soft_clusters: Optional[np.ndarray] = None,
    output_path: str = "cluster_visualization.png",
    title: str = "HDBSCAN Clustering Visualization"
):
    """
    Visualize clusters using UMAP dimensionality reduction.
    
    Args:
        embeddings: Array of shape (n_samples, n_features)
        labels: Hard cluster labels from HDBSCAN (-1 for noise)
        entry_ids: List of entry IDs
        soft_clusters: Optional soft cluster membership probabilities
        output_path: Path to save the visualization
        title: Title for the plot
    """
    if not MATPLOTLIB_AVAILABLE:
        print("WARNING: matplotlib not available. Install with: pip install matplotlib")
        return
    
    if not UMAP_AVAILABLE:
        print("WARNING: umap-learn not available. Install with: pip install umap-learn")
        return
    
    print("\n--- Generating Visualization ---")
    print("Running UMAP dimensionality reduction...")
    
    # Use UMAP to reduce to 2D
    reducer = umap.UMAP(
        n_components=2,
        n_neighbors=15,
        min_dist=0.1,
        metric='euclidean',
        random_state=42
    )
    embedding_2d = reducer.fit_transform(embeddings)
    
    # Create figure with subplots
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    # Get unique labels and create color map
    unique_labels = sorted(set(labels))
    n_clusters = len([l for l in unique_labels if l >= 0])
    
    # Color palette: gray for noise, cycle through colormaps for many clusters
    base_colors = list(cm.tab10.colors) + list(cm.tab20.colors) + list(cm.Set3.colors)
    # Ensure we have enough colors by cycling
    cluster_colors = []
    for i in range(n_clusters):
        cluster_colors.append(base_colors[i % len(base_colors)])
    colors = ['#CCCCCC'] + cluster_colors  # Gray for noise, then cluster colors
    
    # Plot 1: Hard clustering
    ax1 = axes[0]
    for i, label in enumerate(unique_labels):
        mask = labels == label
        color = colors[0] if label == -1 else colors[label + 1]
        alpha = 0.3 if label == -1 else 0.8
        marker_size = 20 if label == -1 else 50
        label_text = "Noise" if label == -1 else f"Cluster {label}"
        
        ax1.scatter(
            embedding_2d[mask, 0],
            embedding_2d[mask, 1],
            c=[color],
            alpha=alpha,
            s=marker_size,
            label=f"{label_text} ({mask.sum()})"
        )
    
    ax1.set_title(f"Hard Clustering\n{n_clusters} clusters, {(labels == -1).sum()} noise points")
    ax1.set_xlabel("UMAP 1")
    ax1.set_ylabel("UMAP 2")
    ax1.legend(loc='best', fontsize=8)
    
    # Plot 2: Soft clustering (membership strength) or cluster probabilities
    ax2 = axes[1]
    if soft_clusters is not None and len(soft_clusters.shape) > 1 and soft_clusters.shape[1] > 0:
        # Color by max membership probability
        max_probs = soft_clusters.max(axis=1)
        scatter = ax2.scatter(
            embedding_2d[:, 0],
            embedding_2d[:, 1],
            c=max_probs,
            cmap='viridis',
            alpha=0.7,
            s=30
        )
        plt.colorbar(scatter, ax=ax2, label='Max Cluster Membership')
        ax2.set_title("Soft Clustering\n(Color = max membership probability)")
    else:
        # Just show the HDBSCAN probabilities if available
        scatter = ax2.scatter(
            embedding_2d[:, 0],
            embedding_2d[:, 1],
            c=labels,
            cmap='tab10',
            alpha=0.7,
            s=30
        )
        ax2.set_title("Cluster Labels")
    
    ax2.set_xlabel("UMAP 1")
    ax2.set_ylabel("UMAP 2")
    
    plt.suptitle(title, fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    # Save figure
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"Visualization saved to: {output_path}")
    
    # Also create a condensed tree visualization if available
    plt.close()
    
    return output_path


def visualize_condensed_tree(
    clusterer: hdbscan.HDBSCAN,
    output_path: str = "condensed_tree.png"
):
    """
    Visualize the HDBSCAN condensed tree.
    
    Args:
        clusterer: Fitted HDBSCAN clusterer
        output_path: Path to save the visualization
    """
    if not MATPLOTLIB_AVAILABLE:
        return
    
    try:
        fig, ax = plt.subplots(figsize=(12, 8))
        clusterer.condensed_tree_.plot(
            select_clusters=True,
            selection_palette=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
                              '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'],
            axis=ax
        )
        ax.set_title("HDBSCAN Condensed Tree\n(Selected clusters highlighted)")
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"Condensed tree saved to: {output_path}")
        plt.close()
    except Exception as e:
        print(f"Could not generate condensed tree: {e}")


def query_entry_clusters(entry_id: int, metadata_db: Optional[str] = None):
    """Query clusters for a specific entry."""
    meta_db = ClusterMetadataDB()
    clusters = meta_db.get_entry_clusters(entry_id)
    meta_db.close()
    
    print(f"\nClusters for Entry {entry_id}:")
    for c in clusters:
        primary = " (primary)" if c['is_primary'] else ""
        print(f"  Cluster {c['cluster_id']}: {c['probability']:.3f}{primary}")
    
    return clusters


def query_cluster_entries(cluster_id: int, metadata_db: Optional[str] = None):
    """Query entries in a specific cluster."""
    meta_db = ClusterMetadataDB()
    entries = meta_db.get_cluster_entries(cluster_id)
    meta_db.close()
    
    print(f"\nEntries in Cluster {cluster_id}:")
    for e in entries:
        primary = " (primary)" if e['is_primary'] else ""
        print(f"  Entry {e['entry_id']}: {e['probability']:.3f}{primary}")
    
    return entries


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="HDBSCAN Clustering for Journal Entries",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run with recommended defaults (UMAP + leaf selection)
  python hdbscan_clustering.py --user-id 1 --visualize

  # Generate topic labels using SmolLM2
  python hdbscan_clustering.py --user-id 1 --generate-topics

  # Fine-tune for more/fewer clusters
  python hdbscan_clustering.py --min-cluster-size 3 --min-samples 1  # More clusters
  python hdbscan_clustering.py --min-cluster-size 8 --min-samples 3  # Fewer clusters

  # Disable UMAP (not recommended for high-dim embeddings)
  python hdbscan_clustering.py --no-umap

  # Use EOM selection for larger, broader clusters
  python hdbscan_clustering.py --cluster-selection-method eom
        """
    )
    parser.add_argument("--user-id", type=int, default=1, help="User ID to cluster")
    parser.add_argument("--limit", type=int, default=0, help="Maximum number of entries (0=all)")
    
    # HDBSCAN parameters
    parser.add_argument("--min-cluster-size", type=int, default=5, help="Minimum cluster size (default: 5)")
    parser.add_argument("--min-samples", type=int, default=2, help="Minimum samples in neighborhood (default: 2)")
    parser.add_argument("--membership-threshold", type=float, default=0.1, help="Soft membership threshold (default: 0.1)")
    parser.add_argument("--cluster-selection-epsilon", type=float, default=0.0, help="Cluster merging distance (default: 0.0)")
    parser.add_argument("--cluster-selection-method", type=str, default="leaf", choices=["leaf", "eom"],
                       help="Cluster selection: 'leaf' (fine-grained) or 'eom' (larger) (default: leaf)")
    parser.add_argument("--metric", type=str, default="euclidean", help="HDBSCAN distance metric (default: euclidean)")
    
    # UMAP parameters
    parser.add_argument("--no-umap", action="store_true", help="Disable UMAP dimensionality reduction")
    parser.add_argument("--umap-components", type=int, default=10, help="UMAP output dimensions (default: 10)")
    parser.add_argument("--umap-metric", type=str, default="cosine", help="UMAP distance metric (default: cosine)")
    
    # Topic generation parameters
    parser.add_argument("--generate-topics", action="store_true", 
                       help="Generate topic labels using SmolLM2")
    parser.add_argument("--topic-model", type=str, default="HuggingFaceTB/SmolLM2-135M-Instruct",
                       help="Hugging Face model for topic labeling (default: HuggingFaceTB/SmolLM2-135M-Instruct)")
    
    # Other options
    parser.add_argument("--metadata-db", type=str, default=None, help="Deprecated: metadata is now stored in PostgreSQL")
    parser.add_argument("--query-entry", type=int, help="Query clusters for an entry ID")
    parser.add_argument("--query-cluster", type=int, help="Query entries in a cluster ID")
    parser.add_argument("--visualize", action="store_true", help="Generate visualization plots")
    parser.add_argument("--output-dir", type=str, default=".", help="Output directory for visualizations")
    
    args = parser.parse_args()
    
    # Handle limit: 0 means all entries (None)
    limit = None if args.limit == 0 else args.limit
    
    if args.query_entry:
        query_entry_clusters(args.query_entry, args.metadata_db)
    elif args.query_cluster is not None:
        query_cluster_entries(args.query_cluster, args.metadata_db)
    else:
        run_clustering(
            user_id=args.user_id,
            min_cluster_size=args.min_cluster_size,
            min_samples=args.min_samples,
            membership_threshold=args.membership_threshold,
            cluster_selection_epsilon=args.cluster_selection_epsilon,
            metric=args.metric,
            cluster_selection_method=args.cluster_selection_method,
            use_umap=not args.no_umap,
            umap_n_components=args.umap_components,
            umap_metric=args.umap_metric,
            generate_topics=args.generate_topics,
            topic_model=args.topic_model,
            metadata_db=args.metadata_db,
            limit=limit,
            visualize=args.visualize,
            output_dir=args.output_dir
        )
